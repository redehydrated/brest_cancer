{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794b7c61",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Preparation\n",
    "\n",
    "This notebook is dedicated to preparing the Breast Cancer Wisconsin dataset for further analysis and modeling. The steps include loading the dataset, basic data exploration, data cleaning, splitting the dataset into training and testing sets, and feature scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166a7ca",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "### Summary of Insights\n",
    "\n",
    "In this notebook, we have successfully prepared the Breast Cancer Wisconsin dataset for further analysis and modeling. The key steps and insights include:\n",
    "\n",
    "1. **Data Loading and Initial Exploration:**\n",
    "   - We loaded the dataset and performed basic data exploration to understand its structure and the distribution of key features. This helped us identify the primary features and the target variable that will be used for classification.\n",
    "\n",
    "2. **Visualization of Feature Distributions:**\n",
    "   - Through scatter plots, we visualized the relationship between the mean, standard error, and worst values of each feature. This provided insights into how malignant and benign tumors differ in terms of size, variability, and extremity. Notably, malignant tumors tend to have smaller and more uniform features, consistent with their poorly differentiated nature.\n",
    "\n",
    "3. **Data Splitting:**\n",
    "   - The dataset was split into training, validation, and test sets using a stratified approach to ensure that the class distribution is consistent across all sets. This will allow for reliable model training and evaluation.\n",
    "\n",
    "4. **Normalization:**\n",
    "   - We applied Z-score normalization (standardization) to the feature data, ensuring that all features have a mean of zero and a standard deviation of one. This step is crucial for the performance of many machine learning algorithms.\n",
    "\n",
    "### Plans for Exploratory Data Analysis (EDA) and Model Development\n",
    "\n",
    "With the data now preprocessed and ready, the next steps will focus on exploratory data analysis (EDA) and model development:\n",
    "\n",
    "1. **Clustering Analysis in EDA:**\n",
    "   - We will perform clustering analysis to explore natural groupings in the data. This might reveal patterns or subgroups within the data that are not immediately apparent, which could inform feature engineering or model selection.\n",
    "\n",
    "2. **Feature Selection and Engineering:**\n",
    "   - Based on the insights gained from EDA, we will consider selecting or engineering features that have the most predictive power or that help differentiate between malignant and benign tumors.\n",
    "\n",
    "3. **Model Development:**\n",
    "   - We will develop and evaluate various machine learning models, starting with simpler models like logistic regression and moving towards more complex ones such as random forests or support vector machines. The goal will be to find the model that best balances accuracy with interpretability.\n",
    "\n",
    "4. **Hyperparameter Tuning and Model Validation:**\n",
    "   - Using the validation set, we will fine-tune model hyperparameters to optimize performance. The test set will then be used to evaluate the final model's performance and ensure that it generalizes well to unseen data.\n",
    "\n",
    "This structured approach will help us build a robust and reliable model for classifying breast cancer tumors, ultimately contributing to better diagnostic tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969d9d1",
   "metadata": {},
   "source": [
    "### Clarification on the Data and Features\n",
    "\n",
    "The Breast Cancer Wisconsin dataset is based on digitized images of fine needle aspirates (FNA) of breast masses. These images are essentially close-ups of clusters of cells taken from a breast tumor. The features in the dataset are derived from the nuclei of these cells, rather than from individual cells.\n",
    "\n",
    "\n",
    "Images: Each image consists of a cluster of cells from the tumor, and the features describe properties of the nuclei within these cells.\n",
    "Features: The features are calculated for the nuclei of cells within the cluster, and they are summarized for the entire image (sample). Some features are measured directly, while others are derived metrics (e.g., ratios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed4a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5941ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.display.max_columns\n",
    "pd.options.display.max_columns = 40\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f62e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
    "df['target'] = cancer_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ee223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Basic Data Understanding\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0ecac",
   "metadata": {},
   "source": [
    "### Purpose: \n",
    "The dataset was originally compiled for the purpose of studying the use of cytological features of breast FNA images to differentiate between malignant and benign tumors. The aim was to develop diagnostic tools that could assist medical professionals in accurately identifying cancerous tissues, potentially reducing the need for invasive surgical procedures.\n",
    "\n",
    "### Data Source: \n",
    "The features in the dataset were extracted from digitized images of breast FNA samples, which capture the visual properties of the cell nuclei present in the aspirate. Each sample consists of measurements related to the nuclei's size, shape, texture, and other characteristics.\n",
    "\n",
    "### Features\n",
    "The dataset contains 569 instances, each representing a breast mass, with the following features:\n",
    "\n",
    "- ID Number: A unique identifier for each case (not used for modeling). \n",
    "- Diagnosis: The target variable, indicating whether the tumor is benign (B) or malignant (M).\n",
    "- 30 Numeric Features: These features are computed from the image of each cell nucleus and include:\n",
    " - Radius (mean, standard error, worst): Mean of distances from the center to points on the perimeter.\n",
    " - Texture (mean, standard error, worst): Standard deviation of grayscale values.\n",
    " - Perimeter (mean, standard error, worst): Perimeter of the nuclei.\n",
    " - Area (mean, standard error, worst): Area of the nuclei.\n",
    " - Smoothness (mean, standard error, worst): Local variation in radius lengths.\n",
    " - Compactness (mean, standard error, worst): Perimeter^2 / area - 1.0.\n",
    " - Concavity (mean, standard error, worst): Severity of concave portions of the contour.\n",
    " - Concave points (mean, standard error, worst): Number of concave portions of the contour.\n",
    " - Symmetry (mean, standard error, worst): Symmetry of the nuclei.\n",
    " - Fractal dimension (mean, standard error, worst): \"Coastline approximation\" - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bba0c4",
   "metadata": {},
   "source": [
    "### Checking for Missing Values\n",
    "\n",
    "Next, we check for any missing values in the dataset. Fortunately, this dataset is clean, with no missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db055cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25758d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nSummary statistics of the dataset:\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace83dd",
   "metadata": {},
   "source": [
    "1. Data Distribution:\n",
    "The mean values of features like mean radius, mean texture, mean perimeter, and mean area are significantly higher than the rest of the features, indicating a larger range and variability in these measurements.\n",
    "The standard deviations (std) show that there is a wide spread in the data, particularly for features like mean perimeter, mean area, and the worst values of these features.\n",
    "2. Outliers:\n",
    "Several features have a wide range, with the maximum values being much higher than the 75th percentile. This is particularly evident in mean area, mean perimeter, and their corresponding worst values, suggesting the presence of outliers. This is further supported by the box plot, where some features have many outlier points.\n",
    "Features like mean smoothness, mean compactness, mean concavity, and mean fractal dimension have much smaller ranges, with less variability and fewer outliers.\n",
    "3. Central Tendency and Spread:\n",
    "The median (50% percentile) values are generally closer to the lower quartile (25%) than the upper quartile (75%) for most features, indicating a right-skewed distribution for these features. This suggests that the majority of the data points are clustered towards the lower end of the range.\n",
    "Features related to concavity and concave points show higher variability, with their upper quartiles and maximum values indicating some samples with significantly higher concavity compared to others.\n",
    "4. Relationship Between Features:\n",
    "Features like mean radius, mean perimeter, and mean area are likely to be highly correlated, as they describe related geometric properties of the tumors.\n",
    "5. Target Variable Distribution:\n",
    "The target variable has a binary distribution, with 0 representing benign and 1 representing malignant tumors. The summary statistics show the counts of each class, which might suggest a slight imbalance that should be considered during model development.\n",
    "6. Feature Scaling Needs:\n",
    "Given the wide range of values in different features (e.g., mean area ranges from 143.5 to 2501, while mean smoothness ranges from 0.0526 to 0.1634), scaling will be necessary to ensure that no single feature dominates the learning algorithm due to its magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d489af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nSummary statistics of the dataset in target split:\")\n",
    "df.groupby('target').agg(['mean', 'median', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d004ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_features = [\n",
    "    'radius', 'texture', 'perimeter', 'area', 'smoothness',\n",
    "    'compactness', 'concavity', 'concave points', 'symmetry', 'fractal dimension'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9f411",
   "metadata": {},
   "source": [
    "#### Visualizing Feature Distributions in the Breast Cancer Wisconsin Dataset\n",
    "\n",
    "In this section, we explore the distributions of various features in the Breast Cancer Wisconsin dataset. Each subplot presents a scatter plot where:\n",
    "- The **x-axis** shows the mean value of a specific feature (e.g., radius, texture, perimeter).\n",
    "- The **y-axis** represents the standard error of that feature, indicating the variability within the sample.\n",
    "- The **size of each point** corresponds to the worst (maximum) value of the feature within each sample.\n",
    "- Points are color-coded by the target variable, indicating whether the tumor is benign (blue) or malignant (orange).\n",
    "\n",
    "This visualization helps us examine how the central tendency, variability, and extremity of each feature vary between benign and malignant tumors, providing insights into which features might be most useful for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bed6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a figure and a grid of subplots\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each feature and create a scatter plot\n",
    "for i, feat in enumerate(main_features):\n",
    "    sns.scatterplot(\n",
    "        data=df, x=f'mean {feat}', y=f'{feat} error', hue='target',\n",
    "        size=f'worst {feat}', ax=axes[i], legend=True\n",
    "    )\n",
    "    axes[i].set_title(f'{feat.capitalize()} Distribution')\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.15, 1), title=\"Legend\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fb061a",
   "metadata": {},
   "source": [
    "## Actionable Insights from Feature Distributions\n",
    "\n",
    "The scatter plots reveal several key insights:\n",
    "\n",
    "1. **Smaller Size of Malignant Tumors:**\n",
    "   - Contrary to what might be expected, **malignant tumors (orange)** tend to have smaller mean values for features such as **radius**, **perimeter**, and **area** compared to benign tumors (blue). This suggests that malignant tumors in this dataset are characterized by smaller cell nuclei or areas, which is a crucial finding for feature selection.\n",
    "\n",
    "2. **Lower Variability in Malignant Tumors:**\n",
    "   - Malignant tumors often show lower variability (standard error) in several features, including **smoothness** and **symmetry**. This consistency might indicate that malignant cells are more uniform in structure compared to benign cells, which show greater variability.\n",
    "\n",
    "3. **Outliers and Extremes:**\n",
    "   - In features like **area** and **concavity**, some malignant tumors exhibit extreme values (larger points), suggesting that while many malignant tumors are smaller, there are cases where they exhibit unusually large or aggressive characteristics.\n",
    "\n",
    "4. **Overlap in Some Features:**\n",
    "   - Features such as **texture** and **fractal dimension** show considerable overlap between benign and malignant tumors, making them less effective for distinguishing between the two classes on their own. However, they might still be valuable in combination with other features.\n",
    "\n",
    "5. **Potential Feature Importance:**\n",
    "   - Features like **radius**, **perimeter**, and **concave points** show noticeable separation between benign and malignant tumors in terms of their mean values, which could make them strong candidates for feature importance in a classification model.\n",
    "\n",
    "These insights suggest that smaller and more uniform features are characteristic of malignant tumors in this dataset. This understanding can guide further steps in feature engineering and model development, particularly in focusing on the features that show the most distinct separation between benign and malignant cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67933d9f",
   "metadata": {},
   "source": [
    "### Additional Insights on Poor Differentiation and Cancer Cell Characteristics\n",
    "\n",
    "The visualization reveals that malignant tumors tend to have smaller and more uniform features, which is consistent with the fact that cancer cells are often poorly differentiated. Poorly differentiated cells are typically less structured and lack the specialized functions of normal cells, making them appear more uniform and smaller under a microscope. This characteristic is associated with higher-grade tumors that are more aggressive and have a worse prognosis.\n",
    "\n",
    "This finding is supported by scientific literature, which indicates that poorly differentiated tumors, due to their lack of specialization, tend to be smaller and more homogeneous in their cellular architecture. This reinforces the importance of focusing on these features during model development, as they are key indicators of malignancy in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45424d",
   "metadata": {},
   "source": [
    "## Creating Train, Validation, and Test Samples\n",
    "\n",
    "To ensure robust model evaluation and development, we will split our dataset into three parts: training, validation, and test sets. The training set will be used to train our models, the validation set will help in tuning hyperparameters and model selection, and the test set will provide an unbiased evaluation of the final model's performance.\n",
    "\n",
    "Given the class imbalance in our dataset (with more benign cases than malignant), we will perform a stratified split. This ensures that each set (train, validation, and test) maintains the same class distribution as the original dataset, which is crucial for accurate and representative model performance assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1830fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into train + validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    df.drop('target', axis=1), df['target'], test_size=0.2, random_state=1010101, stratify=df['target']\n",
    ")\n",
    "\n",
    "# Now split the train + validation set into separate train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=1010101, stratify=y_train_val\n",
    ")\n",
    "\n",
    "# Verify the distribution of the target variable in each set\n",
    "print(\"Training set distribution:\\n\", pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"Validation set distribution:\\n\", pd.Series(y_val).value_counts(normalize=True))\n",
    "print(\"Test set distribution:\\n\", pd.Series(y_test).value_counts(normalize=True))\n",
    "\n",
    "# Save the splits for later use\n",
    "np.savez_compressed('train_val_test_split.npz', \n",
    "                    X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "                    y_train=y_train, y_val=y_val, y_test=y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd626fac",
   "metadata": {},
   "source": [
    "## Normalization of Features\n",
    "\n",
    "Normalization is a crucial step in preparing the data for machine learning models, especially when features have different scales. In our dataset, features like area, radius, and perimeter have significantly larger scales compared to features like smoothness and symmetry. Without normalization, these large-scale features could disproportionately influence the model's learning process.\n",
    "\n",
    "We will apply **z-score normalization** (also known as standardization), which transforms the data such that each feature has a mean of 0 and a standard deviation of 1. This is done by subtracting the mean of each feature and dividing by its standard deviation. This approach ensures that each feature contributes equally to the model's training process, enhancing the model's performance and convergence during training.\n",
    "\n",
    "### Why Z-score Normalization?\n",
    "- **Consistency:** It ensures all features are on the same scale, making the model's gradient descent optimization process more efficient.\n",
    "- **Model Performance:** Many machine learning models, such as logistic regression and support vector machines, assume or perform better when the data is standardized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the fitted scaler to transform the validation and test data\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verify the transformation by checking the mean and standard deviation of the scaled data\n",
    "print(\"Training set mean after scaling:\\n\", np.mean(X_train_scaled, axis=0))\n",
    "print(\"Training set standard deviation after scaling:\\n\", np.std(X_train_scaled, axis=0))\n",
    "\n",
    "# Save the normalized datasets\n",
    "np.savez_compressed('normalized_data.npz', \n",
    "                    X_train_scaled=X_train_scaled, \n",
    "                    X_val_scaled=X_val_scaled, \n",
    "                    X_test_scaled=X_test_scaled,\n",
    "                    y_train=y_train, y_val=y_val, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fdcec",
   "metadata": {},
   "source": [
    "Training set mean after scaling: The means of the features are all extremely close to zero, which is what we expect after standardization.\n",
    "\n",
    "Training set standard deviation after scaling: The standard deviations are all 1, which confirms that the data has been normalized correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b57571",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "### Summary of Insights\n",
    "\n",
    "In this notebook, we have successfully prepared the Breast Cancer Wisconsin dataset for further analysis and modeling. The key steps and insights include:\n",
    "\n",
    "1. **Data Loading and Initial Exploration:**\n",
    "   - We loaded the dataset and performed basic data exploration to understand its structure and the distribution of key features. This helped us identify the primary features and the target variable that will be used for classification.\n",
    "\n",
    "2. **Visualization of Feature Distributions:**\n",
    "   - Through scatter plots, we visualized the relationship between the mean, standard error, and worst values of each feature. This provided insights into how malignant and benign tumors differ in terms of size, variability, and extremity. Notably, malignant tumors tend to have smaller and more uniform features, consistent with their poorly differentiated nature.\n",
    "\n",
    "3. **Data Splitting:**\n",
    "   - The dataset was split into training, validation, and test sets using a stratified approach to ensure that the class distribution is consistent across all sets. This will allow for reliable model training and evaluation.\n",
    "\n",
    "4. **Normalization:**\n",
    "   - We applied Z-score normalization (standardization) to the feature data, ensuring that all features have a mean of zero and a standard deviation of one. This step is crucial for the performance of many machine learning algorithms.\n",
    "\n",
    "### Plans for Exploratory Data Analysis (EDA) and Model Development\n",
    "\n",
    "With the data now preprocessed and ready, the next steps will focus on exploratory data analysis (EDA) and model development:\n",
    "\n",
    "1. **Clustering Analysis in EDA:**\n",
    "   - We will perform clustering analysis to explore natural groupings in the data. This might reveal patterns or subgroups within the data that are not immediately apparent, which could inform feature engineering or model selection.\n",
    "\n",
    "2. **Feature Selection and Engineering:**\n",
    "   - Based on the insights gained from EDA, we will consider selecting or engineering features that have the most predictive power or that help differentiate between malignant and benign tumors.\n",
    "\n",
    "3. **Model Development:**\n",
    "   - We will develop and evaluate various machine learning models, starting with simpler models like logistic regression and moving towards more complex ones such as random forests or support vector machines. The goal will be to find the model that best balances accuracy with interpretability.\n",
    "\n",
    "4. **Hyperparameter Tuning and Model Validation:**\n",
    "   - Using the validation set, we will fine-tune model hyperparameters to optimize performance. The est set will then be used to evaluate the final model's performance and ensure that it generalizes well to unseen data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
